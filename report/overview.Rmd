# Performance Results

The last complete run of this benchmark setup yielded the results presented
below. This report was generated on `r Sys.time()`.

```{r load-scripts, echo=FALSE, include=FALSE}
# load libraries, the data, and prepare it
if (Sys.getenv("RSTUDIO") == "1") { setwd("/home/guido/Documents/Projects/are-we-fast-yet/report") }
source("scripts/libraries.R", chdir=TRUE)
opts_chunk$set(dev = 'png',
               dev.args=list(pointsize=10),
               echo = FALSE,
               fig.keep='all',
               fig.path="figures/",
               external=FALSE,
               tidy=FALSE)
#    cache=TRUE,

vm_names <- c("Java8U66"              = "Java",
              "SOMns-Enterprise"      = "SOMns",
#              "TruffleSOM-TOM-Enterprise" = "TruffleSOMTOM",
              "TruffleSOMMate-Enterprise" = "TruffleSOM",
              "TruffleMate-Enterprise" = "TruffleMATE",
              "GraalJS"               = "Graal.js",
              "Node"                  = "Node.js",
              "Crystal"               = "Crystal",
              "JRubyJ8"               = "JRuby",
              "JRubyTruffleEnterprise" = "JRuby+Truffle",
              "MRI23"                 = "MRI",
              "RBX314"                = "Rubinius",
              "Pharo"                 = "Pharo")


vms_all  <- names(vm_names)
vms_slowJit  <- c("Pharo", "TruffleMate-Enterprise")
vms_slow <- c("RBX314", "MRI23", "JRubyJ8")
vms_fast <- c("Crystal", "GraalJS", "JRubyTruffleEnterprise",
              "Java8U66", "Node", "SOMns-Enterprise",
              #"TruffleSOM-TOM-Enterprise",
              "TruffleSOMMate-Enterprise")
vms_truffle <- c("GraalJS", "JRubyTruffleEnterprise",
                 "SOMns-Enterprise", "TruffleSOM-TOM-Enterprise", "TruffleSOMMate-Enterprise")

vms_JIT <- append(vms_fast, vms_slowJit)
vms_JIT <- setdiff(vms_JIT, c("Java8U66"))

vms_JournalMateJSS <- c("TruffleSOMMate-Enterprise", "Pharo", "TruffleMate-Enterprise", "Node")

assert_that(all(sort(c(vms_slow, vms_fast, vms_slowJit)) == sort(vms_all))) ## sanity check

vm_colors <- brewer.pal(12, "Paired")  # to replace scale_fill_brewer(type = "qual", palette = "Paired")
names(vm_colors) <- vm_names


data <- load_data_file("data/benchmark.data")
#data_p <- load_data_file("data/pharo.data")
#data <- rbind(data, data_p)
data <- droplevels(subset(data, Benchmark != "DeltaBlueV8" & VM %in% vms_all,
                          select = c(Value, Unit, Benchmark, VM, Iteration)))

data_fast_vms      <- droplevels(subset(data, Iteration >= 1500 & Iteration < 2500 & VM %in% vms_fast))
data_very_slow_vms <- droplevels(subset(data, VM %in% vms_slow & VM != "JRubyJ8"))
data_slow_vms      <- droplevels(subset(data, Iteration >= 100 & (VM == "JRubyJ8" | VM == "Pharo")))
data_slow_vms2    <- droplevels(subset(data, Iteration >= 1500 & Iteration < 2500 & VM == "TruffleMate-Enterprise"))
data <- rbind(data_fast_vms, data_slow_vms, data_very_slow_vms, data_slow_vms2)


norm <- ddply(data, ~ Benchmark, transform,
              RuntimeRatio = Value / mean(Value[VM == "Java8U66"]))
stats <- ddply(norm, ~ VM + Benchmark, summarise,
               Time.ms = mean(Value),
               sd      = sd(Value),
               RuntimeFactor = geometric.mean(RuntimeRatio),
               RR.sd         = sd(RuntimeRatio),
               RR.median     = median(RuntimeRatio))
stats <- ddply(stats, ~ VM, transform,
    VMMean = geometric.mean(RuntimeFactor),
    min = min(RuntimeFactor),
    max = max(RuntimeFactor))

plot_benchmarks_speedup_for_vms <- function(stats, vms) {
  vm_stats <- droplevels(subset(stats, VM %in% vms))

  for (b in levels(vm_stats$Benchmark)) {
    data_b <- droplevels(subset(vm_stats, Benchmark == b))

    p <- ggplot(data_b, aes(x = VM, y = RuntimeFactor, fill = VM)) +
      geom_bar(stat = "identity") +
      geom_errorbar(aes(ymax = RuntimeFactor + RR.sd, ymin = RuntimeFactor - RR.sd), width=0.25) +
      coord_flip() + theme_bw() + # scale_fill_manual(values=col) +
      theme(legend.position="none") + ggtitle(b)
    tryCatch({print(p)})
  }
}

plot_benchmarks_speedup_for_vms_faceted <- function(
  stats, vms, ylab = "Runtime Factor, normalized to Java\n(lower is better)") {
  vm_stats <- subset(stats, VM %in% vms)
  vm_stats$VM <- revalue(vm_stats$VM, vm_names)
  vm_stats$VM <- reorder(vm_stats$VM, X=vm_stats$VMMean)
  breaks <- levels(droplevels(vm_stats)$VM)
  col_values <- sapply(breaks, function(x) vm_colors[[x]])

  p <- ggplot(vm_stats, aes(x = VM, y = RuntimeFactor, fill = VM)) +
      geom_bar(stat = "identity") +
      geom_errorbar(aes(ymax = RuntimeFactor + RR.sd, ymin = RuntimeFactor - RR.sd), width=0.25) +
      facet_wrap(~ Benchmark, ncol = 1, scales="free_y") +
       theme_bw() + theme_simple(font_size = 8) + # scale_fill_manual(values=col) + coord_flip() +
      theme(legend.position="none", axis.text.x=element_text(angle=90, hjust = 1, vjust = 0.5)) +
    scale_fill_manual(values = col_values) +
    ylab(ylab)
  print(p)
}

overview_box_plot <- function(stats, vms, prepare_data = NULL, pre_plot = NULL) {
  vm_stats <- subset(stats, VM %in% vms)
  vm_stats$VM <- revalue(vm_stats$VM, vm_names)
  vm_stats$VM <- reorder(vm_stats$VM, X=-vm_stats$VMMean)
  if (!is.null(prepare_data)) {
   vm_stats <- prepare_data(vm_stats)
  }

  breaks <- levels(droplevels(vm_stats)$VM)
  col_values <- sapply(breaks, function(x) vm_colors[[x]])

  plot <- ggplot(vm_stats, aes(x=VM, y=RuntimeFactor, fill = VM))
  if (!is.null(pre_plot)) {
    plot <- pre_plot(plot)
  }
  plot <- plot +
    geom_boxplot(outlier.size = 0.5) + #fill=get_color(5, 7)
    theme_bw() + theme_simple(font_size = 8) +
    theme(axis.text.x = element_text(angle= 90, vjust=0.5, hjust=1), legend.position="none", plot.title = element_text(hjust = 0.5), aspect.ratio=0.5) +
    #scale_y_log10(breaks=c(1,2,3,10,20,30,50,100,200,300,500,1000)) + #limit=c(0,30), breaks=seq(0,100,5), expand = c(0,0)
    #+ coord_flip()
    ggtitle("Runtime Factor, normalized to Java (lower is better)") + xlab("") +
    scale_fill_manual(values = col_values)
  plot
}
```

All results are normalized to Java 1.8.0_91. Furthermore, we report peak
performance. This means, the reported measurements are taken after warmup and
compilation of the benchmark code is completed.

## Overview

##### Fast Language Implementations

The following set of language implementations reaches the performance of Java on
our set of benchmarks within a factor of 2 to 3 on average. To allow for a more
detailed assessment of these *fast* language implementations, we exclude slower
ones from the following plot.

```{r fast-langs-overview, fig.width=15, fig.height=3}
p <- overview_box_plot(stats, vms_JournalMateJSS)
p + scale_y_continuous(limit=c(0,18), breaks = c(1, 2, 3, 4, 6, 8, 10, 12, 14), na.value = 50)
```

##### All Language Implementations

Other language implementations are not necessarily reaching performance similar
to Java on our benchmarks. The following plot include all of the
implementations.

```{r all-langs-overview, fig.width=8, fig.height=3}
p <- overview_box_plot(stats, vms_JIT)
p + scale_y_continuous(breaks = c(0, 10, seq(from=10, by=20, to=30)))
```

##### Performance Overview Data
<a id="data-table"></a>

The following table contains the numerical representation of the results
depicted above.

```{r truffle-lang-table, results='asis', echo=FALSE}
vm_stats <- ddply(stats, ~ VM, summarise,
                     geomean = geometric.mean(RuntimeFactor),
                     sd      = sd(RuntimeFactor),
                     min     = min(RuntimeFactor),
                     max     = max(RuntimeFactor),
                     median  = median(RuntimeFactor))
vm_stats$VM <- revalue(vm_stats$VM, vm_names)
vm_stats$VM <- reorder(vm_stats$VM, X=vm_stats$geomean)


t <- tabular(Justify("l")*Heading()*VM ~
             Heading('Runtime Factor over Java')*Justify("r")*Format(sprintf("%.2f"))*((geomean + sd + min + max + median)*Heading()*identity), data=vm_stats)
table_options(justification="c ")
html(t)
```

## Details for all Benchmarks
<a id="all-benchmarks"></a>

The following plots show results for each of the benchmarks.

##### Fast Language Implementations

```{r fast-langs-benchmarks, fig.width=4, fig.height=16}
plot_benchmarks_speedup_for_vms_faceted(stats, vms_fast)
```

##### Slow Language Implementations

```{r slow-langs-benchmarks, fig.width=4, fig.height=16}
plot_benchmarks_speedup_for_vms_faceted(stats, vms_slow)
```

##### Benchmark Results
<a id="benchmark-table"></a>

The following table contains the numerical results for all benchmarks.

```{r benchmark-table, results='asis', echo=FALSE}
t_stats <- stats
t_stats$VM <- revalue(t_stats$VM, vm_names)
t_stats$VM <- reorder(t_stats$VM, X=t_stats$VMMean)


t <- tabular(Justify("l")*Heading()*Benchmark*VM ~
             Heading('Runtime Factor over Java')*Justify("r")*Format(sprintf("%.2f"))*((
                 Heading("geomean")*RuntimeFactor
               + Heading("sd")*RR.sd
               # + Heading("median")*RR.median
               )*Heading()*identity), data=t_stats)
html(t)
```
